{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 顔の検出と分析\n",
    "\n",
    "コンピュータビジョンソリューションでは、人間の顔を検出、分析、または識別できるようにするために、人工知能（AI）ソリューションを必要とすることがよくあります。例えば、小売企業のNorthwind Traders社が「スマートストア」の導入を決定したとします。このスマートストアでは、AIサービスが店舗を監視してサポートが必要な顧客を特定し、従業員にサポートを指示します。これを実現する一つの方法は、顔の検出と分析を行うことです。つまり、画像の中に顔があるかどうかを判断し、ある場合はその特徴を分析するのです。\n",
    "\n",
    "![A robot analyzing a face](./images/face_analysis.jpg)\n",
    "\n",
    "## Face cognitive service を使って顔を検出する\n",
    "\n",
    "Northwind Tradersが作りたいスマートストアシステムは、顧客を検知して顔の特徴を分析できる必要があるとします。Microsoft Azureでは、Azure Cognitive Servicesの一部である**Face**を使ってこれを行うことができます。\n",
    "\n",
    "### Cognitive Services リソースを作成する\n",
    "\n",
    "それではまず、Azureサブスクリプションに**Cognitive Services**リソースを作成しましょう。\n",
    "\n",
    "> **補足**: すでにCognitive Servicesリソースがある場合は、Azureポータルでその**クイックスタート**ページを開き、そのキーとエンドポイントを以下のセルにコピーするだけです。そうでない場合は、以下の手順で作成してください。\n",
    "\n",
    "1. 新しいブラウザタブで、Azureポータル（https://portal.azure.com）を開き、Microsoftアカウントでサインインします。\n",
    "2. **&#65291;リソースの作成** ボタンをクリックし、*Cognitive Services*を検索して以下の設定で **Cognitive Services** リソースを作成します。\n",
    "    - **サブスクリプション**: *ご自身のサプスクリプション*\n",
    "    - **リソースグループ**: *既存のリソースグループを選択するか、ユニークな名前で新しいリソースグループを作成します。*\n",
    "    - **リージョン**: *利用可能なリージョンを選択（例:東日本）*\n",
    "    - **名前**: *ユニークな名前を入力*\n",
    "    - **価格レベル**: S0\n",
    "    - **このボックスをオンにすることにより、以下のすべてのご契約条件を読み、同意したものとみなされます**: チェックを入れます。\n",
    "3. デプロイが完了するまでしばらく待ちます。次に、Cognitive Servicesリソースにアクセスし、**Overview**ページで、サービスのキーを管理するリンクをクリックします。クライアントアプリケーションからCognitive Servicesリソースに接続するには、エンドポイントとキーが必要になります。\n",
    "\n",
    "### Cognitive Servicesリソースのキーとエンドポイントの取得\n",
    "\n",
    "Cognitive Servicesリソースを使用するためには、クライアントアプリケーションはそのエンドポイントと認証キーを必要とします。\n",
    "\n",
    "1. Azureポータルで、Cognitive Servicesリソースの**キーとエンドポイント**ページを開き、リソースの**キー1**をコピーして、**YOUR_COG_KEY**を置き換えて、以下のコードに貼り付けます。\n",
    "\n",
    "2. リソースの**エンドポイント**をコピーし、**YOUR_COG_ENDPOINT**を置き換えて、以下のコードに貼り付けます。\n",
    "\n",
    "3. セルの実行<span>&#9655;</span>ボタン（セルの左上）をクリックして、下のセルのコードを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693964655
    }
   },
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cognitive Servicesリソースがあるので、Faceサービスを使用してストア内の人間の顔を検出できます。\n",
    "\n",
    "以下のコードセルを実行して、例をご覧ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693970079
    }
   },
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from python_code import faces\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a face detection client.\n",
    "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検出された顔にはそれぞれ固有のIDが割り当てられているので、アプリケーションは検出された個々の顔を識別することができます。\n",
    "\n",
    "下のセルを実行して、さらに何人かの店員の顔のIDを確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693970447
    }
   },
   "outputs": [],
   "source": [
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顔の属性を分析する\n",
    "\n",
    "Faceは、単に顔を検出するだけではありません。顔の特徴や表情を分析して、年齢や感情の状態を推測することもできます。例えば、以下のコードを実行して、買い物客の顔の属性を分析してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693971321
    }
   },
   "outputs": [],
   "source": [
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces and specified facial attributes\n",
    "attributes = ['age', 'emotion']\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
    "\n",
    "# Display the faces and attributes (code in python_code/faces.py)\n",
    "faces.show_face_attributes(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像に写っているお客様の感情スコアを見ると、お客様は買い物に満足されているようです。\n",
    "\n",
    "## 類似した顔の検出\n",
    "\n",
    "検出された顔ごとに作成された顔IDは、検出された顔を個別に識別するために使用されます。このIDを使って、検出された顔を以前に検出された顔と比較し、似た特徴を持つ顔を見つけることができます。\n",
    "\n",
    "例えば、以下のセルを実行して、ある画像の買い物客と別の画像の買い物客を比較し、一致する顔を見つけることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693972555
    }
   },
   "outputs": [],
   "source": [
    "# Get the ID of the first face in image 1\n",
    "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_1_stream = open(image_1_path, \"rb\")\n",
    "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
    "face_1 = image_1_faces[0]\n",
    "\n",
    "# Get the face IDs in a second image\n",
    "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_2_stream = open(image_2_path, \"rb\")\n",
    "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
    "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
    "\n",
    "# Find faces in image 2 that are similar to the one in image 1\n",
    "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
    "\n",
    "# Show the face in image 1, and similar faces in image 2(code in python_code/face.py)\n",
    "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顔認識\n",
    "\n",
    "これまでに、Faceは顔と顔の特徴を検出し、互いに似ている2つの顔を識別できることがわかりました。さらに一歩進んで、特定の人の顔を認識できるようにFaceを学習させる、「顔認識」ソリューションを実装することもできます。これは、ソーシャルメディアのアプリケーションで友人の写真に自動的にタグ付けしたり、生体認証システムの一部として顔認識を使用するなど、さまざまなシナリオで役立ちます。\n",
    "\n",
    "この仕組みを理解するために、Northwind Traders社が顔認証を利用して、IT部門の許可された従業員だけが安全なシステムにアクセスできるようにしたいと考えているとします。\n",
    "\n",
    "まず、許可された従業員を表す**人のグループ**を作成してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693973492
    }
   },
   "outputs": [],
   "source": [
    "group_id = 'employee_group_id'\n",
    "try:\n",
    "    # Delete group if it already exists\n",
    "    face_client.person_group.delete(group_id)\n",
    "except Exception as ex:\n",
    "    print(ex.message)\n",
    "finally:\n",
    "    face_client.person_group.create(group_id, 'employees')\n",
    "    print ('Group created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**人のグループ**ができたので、グループに入れたい従業員ごとに**人物**を追加し、各人の写真を複数枚登録することで、Faceが各人の顔の特徴を知ることができます。同じ人物でも、ポーズや表情が異なるのが理想的です。\n",
    "\n",
    "ここでは、Wendellという一人の社員を追加し、その社員の写真を3枚登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693976898
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Add a person (Wendell) to the group\n",
    "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
    "\n",
    "# Get photo's of Wendell\n",
    "folder = os.path.join('data', 'face', 'wendell')\n",
    "wendell_pics = os.listdir(folder)\n",
    "\n",
    "# Register the photos\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for pic in wendell_pics:\n",
    "    # Add each photo to person in person group\n",
    "    img_path = os.path.join(folder, pic)\n",
    "    img_stream = open(img_path, \"rb\")\n",
    "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
    "\n",
    "    # Display each image\n",
    "    img = Image.open(img_path)\n",
    "    i +=1\n",
    "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
    "    a.axis('off')\n",
    "    imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人物が追加され、写真が登録されると、今度はFaceに各人物を認識させるためのトレーニングを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693977046
    }
   },
   "outputs": [],
   "source": [
    "face_client.person_group.train(group_id)\n",
    "print('Trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでモデルが学習されたので、そのモデルを使って画像内の認識された顔を識別することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693994820
    }
   },
   "outputs": [],
   "source": [
    "# Get the face IDs in a second image\n",
    "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
    "\n",
    "# Get recognized face names\n",
    "face_names = {}\n",
    "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
    "for face in recognized_faces:\n",
    "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
    "    face_names[face.face_id] = person_name\n",
    "\n",
    "# show recognized faces\n",
    "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## さらに学ぶ\n",
    "\n",
    "Face cognitive serviceについてさらに学びたい場合は [Faceのドキュメント](https://docs.microsoft.com/azure/cognitive-services/face/)を参照してください。\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
