{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 物体検出\n","\n","**物体検出**とは、コンピュータビジョンの一種であり、機械学習モデルを用いて、画像中の物体の個々のインスタンスを分類し、その位置を示す「境界ボックス」を示すように学習します。これは、「画像の分類」（モデルが「この画像は何の画像か」という質問に答える）から、「この画像にはどのような物体があり、どこにあるのか」という質問をモデルに投げかけることができるソリューションを構築することにつながると考えることができます。\n","\n","![A robot identifying fruit](./images/object-detection.jpg)\n","\n","例えば、食料品店では、カメラを使ってベルトコンベアをスキャンし、ベルト上に商品を置いて個別にスキャンしなくても、特定の商品を識別できるような自動レジシステムを実現するために、物体検出モデルを使用することができます。\n","\n","Microsoft Azureの**Custom Vision** Cognitive Serviceは、カスタム物体検出モデルを作成・公開するためのクラウドベースのソリューションを提供します。\n","\n","## Custom Visionリソースを作成する\n","\n","Custom Visionサービスを使用するには、モデルのトレーニングに使用するAzureリソースと、アプリケーションが使用できるようにモデルを公開するリソースが必要です。これらのタスクのそれぞれに同じリソースを使用することもできますし、両方のリソースが同じリージョンに作成されていれば、それぞれに異なるリソースを使用してコストを別々に配分することもできます。どちらか（または両方）のタスクのリソースは、一般的な**Cognitive Services**リソース、または特定の**Custom Vision**リソースとすることができます。以下の手順で、新しい **Custom Vision**リソースを作成してください（既存のリソースがある場合はそれを使用することもできます）。\n","\n","1. 新しいブラウザタブで、[https://portal.azure.com](https://portal.azure.com)のAzureポータルを開き、Azureサブスクリプションに関連付けられたMicrosoftアカウントを使用してサインインします。\n","2. **&#65291;リソースの作成** ボタンを選択し、*custom vision*を検索し、以下の設定で**Custom Vision**リソースを作成します。\n","    - **作成オプション**: 両方\n","    - **サブスクリプション**: *ご自身のサプスクリプション*\n","    - **リソースグループ**: *既存のリソースグループを選択するか、ユニークな名前で新しいリソースグループを作成します。*\n","    - **名前**: *ユニークな名前を入力*\n","    - **リージョン**: *利用可能なリージョンを選択（例:東日本）*\n","    - **トレーニング価格レベル**: F0\n","    - **予測価格レベル**: F0\n","\n","    > **補足**: サブスクリプションにF0のカスタムビジョンサービスがすでにある場合、このサービスには**S0**を選択してください。\n","\n","3. リソースが作成されるまでしばらく待ちます。\n","\n","## Custom Visionプロジェクトを作成する\n","\n","物体検出モデルをトレーニングするには、トレーニングリソースに基づいてCustom Visionプロジェクトを作成する必要があります。そのためには、Custom Visionポータルを使用します。\n","\n","1. 新しいブラウザタブで、[https://customvision.ai](https://customvision.ai)のCustom Visionポータルを開きます。プロンプトが表示されたら、Azureサブスクリプションに関連付けられたMicrosoftアカウントを使用してサインインし、利用規約に同意します。\n","2. 以下の設定で新しいプロジェクトを作成します。\n","    - **Name**: Grocery Detection\n","    - **Description**: Object detection for groceries.\n","    - **Resource**: *前のステップで作成したCustom Visionリソース*\n","    - **Project Types**: Object Detection\n","    - **Domains**: General\n","3. プロジェクトが作成されブラウザーに表示されるまでしばらく待ちます。\n","   \n","## タグを付ける画像を追加する\n","\n","物体検出モデルを学習させるには、モデルに識別させたいクラスを含む画像をアップロードし、各物体インスタンスの境界ボックスを示すタグを付ける必要があります。\n","\n","1. https://aka.ms/fruit-objects からトレーニングイメージが入った圧縮ファイルをダウンロードし、それをローカルコンピューター上で解凍します（**注意**: トレーニングイメージにアクセスできない場合は、一時的な回避策として https://www.github.com にアクセスしたあと、https://aka.ms/fruit-objects にアクセスしてみてください。）\n","2. Custom Visionポータル[https://customvision.ai](https://customvision.ai)内の物体検出プロジェクト _Grocery Detection_ で作業していることを確認し、「**Add Images**」を選択し、解凍したフォルダ内の画像をすべてアップロードします。\n","\n","![Upload downloaded images by clicking add images.](./images/fruit-upload.jpg)\n","\n","3. 画像のアップロードが完了したら、最初の画像を選択して開きます。\n","4. 画像内の任意の物体の上にマウスを持っていくと、下の画像のように自動検出された領域が表示されます。その後、物体を選択し、必要に応じて領域のサイズを変更して物体を囲みます。または、シンプルに物体の周りをドラッグして領域を作ることもできます。\n","\n","![The default region for an object](./images/object-region.jpg)\n","\n","5. リージョンが物体を囲んでいる場合は、適切な物体タイプ（*apple*、*banana*、*orange*）の新しいタグを追加してください（下図参照）。\n","\n","![A tagged object in an image](./images/object-tag.jpg)\n","\n","6. 画像内の他の物体を選択してタグ付けし、必要に応じて領域のサイズを変更し、新しいタグを追加します。\n","\n","![Two tagged objects in an image](./images/object-tags.jpg)\n","\n","7. 右側の **>** リンクを使って次の画像に移動し、その物体にタグを付けます。その後、apple、banana、orangeのタグ付けをしながら、画像コレクション全体を見ていきます。\n","\n","8. 最後の画像へのタグ付けが完了したら、***Image Detail** エディタを閉じ、**Training Images**ページの **Tags** の下にある **Tagged** を選択すると、タグ付けされたすべての画像が表示されます。\n","\n","![Tagged images in a project](./images/tagged-images.jpg)\n","\n","## モデルのトレーニングとテスト\n","\n","プロジェクト内の画像にタグ付けしたので、モデルを学習する準備が整いました。\n","\n","1. Custom Visionプロジェクトで、「**Train**」をクリックし、タグ付けされた画像を使って物体検出モデルをトレーニングします。**Quick Training**を選択します。\n","2. トレーニングが完了するのを待ち（10分ほどかかります）、**Precision(適合率)**、 **Recall(再現率)**、 **mAP(平均適合率)** のパフォーマンス指標を確認します。これらは分類モデルの予測精度を測定するもので、いずれも高い値を示します。\n","3. ページの右上にある「**Quick Test**」をクリックし、「**Image URL**」ボックスに https://aka.ms/apple-orange と入力して、生成された予測結果を確認します。その後、**Quick Test**のウィンドウを閉じます。\n","\n","## 物体検出モデルの公開と利用\n","\n","これで、学習したモデルを公開し、クライアントアプリケーションから利用する準備が整いました。\n","\n","1. **&#128504; Publish** をクリックして、以下の設定でトレーニングされたモデルを公開（パブリッシュ）します。\n","    - **Model name**: detect-produce\n","    - **Prediction Resource**: *先に作成したCustom visionの**予測** 用リソース*.\n","\n","### (!) チェックイン \n","モデル名が正しく**detect-produce**と入力されていることを確認してください。\n","\n","2. 公開できたら、**Performance**ページの右上にある*設定*（&#9881;）アイコンをクリックして、プロジェクトの設定を表示します。次に、**General**（左）の下にある**Project Id**をコピーします。スクロールダウンして、ステップ5の下のコードセルに**YOUR_PROJECT_ID**を置き換えて貼り付けます。\n","\n","> _**補足**: この演習の最初に **Custom Vision** リソースを作成する代わりに **Cognitive Services** リソースを使用した場合、プロジェクト設定の右側からそのキーとエンドポイントをコピーして、下のコードセルに貼り付け、実行して結果を確認することができます。そうでなければ、以下のステップを続けて、Custom Vision予測リソースのキーとエンドポイントを取得します。_\n","\n","3. **Project Settings**ページの左上にある *Projects Gallery* (&#128065;)アイコンをクリックすると、Custom Visionポータルのホーム画面に戻り、プロジェクトが表示されます。\n","\n","4. Custom Visionポータルのホームページの右上にある*設定* (&#9881;)アイコンをクリックして、Custom Visionサービスの設定を表示します。次に、**Resources**の下で、*prediction*リソース（<u>トレーニング用のリソースではありません</u>）を展開し、その**Key**と**Endpoint**の値をステップ5の下のコードセルにコピーし、**YOUR_KEY**と**YOUR_ENDPOINT**を置き換えます。\n","\n","### (!) チェックイン\n","**Custom Vision**リソースを使用している場合、**prediction**リソース（<u>トレーニング用リソースではありません</u>）を使用しましたか？\n","\n","5. セルの左側にある **Run Cell** (&#9655;）ボタンをクリックして、以下のコードセルを実行し、変数にプロジェクトID、キー、エンドポイントの値を設定します。"]},{"cell_type":"code","execution_count":1,"metadata":{"gather":{"logged":1599692485387}},"outputs":[{"name":"stdout","output_type":"stream","text":["Ready to predict using model detect-produce in project 243c9dc4-c24b-4974-95a1-f0011ae6a4d7\n"]}],"source":["project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\n","cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\n","cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\n","model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\n","print('Ready to predict using model {} in project {}'.format(model_name, project_id))"]},{"cell_type":"markdown","metadata":{},"source":["これで、Custom Visionクライアントでキーとエンドポイントを使用して、カスタムビジョンの物体検出モデルに接続できるようになります。\n","\n","次のコードセルを実行します。このコードセルでは、モデルを使用して、画像内の果物を検出します。\n","\n","> **注意**。コードの詳細についてはあまり気にしないでください。このコードはPython SDK for the Custom Vision serviceを使用して、モデルに画像を送信し、検出された物体の予測値を取得します。それぞれの予測は、クラス名（*apple*, *banana*, *orange*）と、予測された物体が画像のどこで検出されたかを示す*bounding box*座標で構成されます。コードは、この情報を使って、画像上の各物体の周りにラベル付きのボックスを描画します。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599692585672}},"outputs":[],"source":["from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n","from msrest.authentication import ApiKeyCredentials\n","from matplotlib import pyplot as plt\n","from PIL import Image, ImageDraw, ImageFont\n","import numpy as np\n","import os\n","%matplotlib inline\n","\n","# Load a test image and get its dimensions\n","test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\n","test_img = Image.open(test_img_file)\n","test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\n","\n","# Get a prediction client for the object detection model\n","credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\n","predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\n","\n","print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\n","\n","# Detect objects in the test image\n","with open(test_img_file, mode=\"rb\") as test_data:\n","    results = predictor.detect_image(project_id, model_name, test_data)\n","\n","# Create a figure to display the results\n","fig = plt.figure(figsize=(8, 8))\n","plt.axis('off')\n","\n","# Display the image with boxes around each detected object\n","draw = ImageDraw.Draw(test_img)\n","lineWidth = int(np.array(test_img).shape[1]/100)\n","object_colors = {\n","    \"apple\": \"lightgreen\",\n","    \"banana\": \"yellow\",\n","    \"orange\": \"orange\"\n","}\n","for prediction in results.predictions:\n","    color = 'white' # default for 'other' object tags\n","    if (prediction.probability*100) > 50:\n","        if prediction.tag_name in object_colors:\n","            color = object_colors[prediction.tag_name]\n","        left = prediction.bounding_box.left * test_img_w \n","        top = prediction.bounding_box.top * test_img_h \n","        height = prediction.bounding_box.height * test_img_h\n","        width =  prediction.bounding_box.width * test_img_w\n","        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\n","        draw.line(points, fill=color, width=lineWidth)\n","        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\n","plt.imshow(test_img)\n"]},{"cell_type":"markdown","metadata":{},"source":["予測結果には、検出された物体と各予測の確率が表示されます。"]}],"metadata":{"interpreter":{"hash":"887aba159db394e56c0af5a7ac88b171e7a8c319799b3662e2d20ed0d5bb4a15"},"kernel_info":{"name":"python3-azureml"},"kernelspec":{"display_name":"Python 3.6 - AzureML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}
