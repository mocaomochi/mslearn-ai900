{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 音声の認識と合成\n","\n","私たちは、人工知能（AI）システムに話しかけることで、音声による応答を期待しながらコミュニケーションを取ることができるようになってきました。\n","\n","![A robot speaking](./images/speech.jpg)\n","\n","**音声認識**（音声言語を解釈するAIシステム）と**音声合成**（音声応答を生成するAIシステム）は、音声対応のAIソリューションの重要な構成要素です。\n","\n","## Cognitive Servicesリソースを作成する\n","\n","音声を解釈して応答するソフトウェアを構築するには、**Speech** Cognitive Serviceを使用することができます。このサービスは、音声言語をテキストに変換する簡単な方法を提供します。\n","\n","まだ作成していない方は、以下の手順でAzureサブスクリプションに**Cognitive Services**リソースを作成してください。\n","\n","> **補足** すでにCognitive Servicesリソースがある場合は、Azureポータルでその**クイックスタート**ページを開き、そのキーとエンドポイントを下のセルにコピーするだけです。そうでない場合は、以下の手順で作成してください。\n","\n","1. 新しいブラウザタブで、Azureポータル（https://portal.azure.com）を開き、Microsoftアカウントでサインインします。\n","2. **&#65291;リソースの作成** ボタンをクリックし、*Cognitive Services*を検索して以下の設定で **Cognitive Services** リソースを作成します。\n","    - **サブスクリプション**: *ご自身のサプスクリプション*\n","    - **リソースグループ**: *既存のリソースグループを選択するか、ユニークな名前で新しいリソースグループを作成します。*\n","    - **リージョン**: *利用可能なリージョンを選択（例:東日本）*\n","    - **名前**: *ユニークな名前を入力*\n","    - **価格レベル**: S0\n","    - **このボックスをオンにすることにより、以下のすべてのご契約条件を読み、同意したものとみなされます**: チェックを入れます。\n","3. デプロイが完了するまでしばらく待ちます。次に、Cognitive Servicesリソースにアクセスし、**Overview**ページで、サービスのキーを管理するリンクをクリックします。クライアントアプリケーションからCognitive Servicesリソースに接続するには、エンドポイントとキーが必要になります。\n","\n","### Cognitive Servicesのキーとロケーションの取得\n","\n","Cognitive Serviceリソースを使用するには、クライアント・アプリケーションは認証キーとロケーションが必要です。\n","\n","1. Azureポータルで、Cognitive Serviceリソースの**キーとエンドポイント**ページで、リソースの**キー1**をコピーして、**YOUR_COG_KEY**を置き換えて、以下のコードに貼り付けます。\n","\n","2. リソースの **場所/地域** をコピーし、**YOUR_COG_LOCATION** を置き換えて、以下のコードに貼り付けます。\n","> **注意**: **キーとエンドポイント**ページに留まり、このページから **場所/地域** の値をコピーしてください（例：_westus_）。場所/地域フィールドの単語の間にはスペースを入れないでください。\n","3. セルの実行<span>&#9655;</span>ボタン（セルの左上）をクリックして、下のセルのコードを実行します。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599695240794}},"outputs":[],"source":["cog_key = 'YOUR_COG_KEY'\n","cog_location = 'YOUR_COG_LOCATION'\n","language = 'en-US'\n","print('Ready to use cognitive services in {} using key {}. Speech language is set to {}.'.format(cog_location, cog_key, language))"]},{"cell_type":"markdown","metadata":{},"source":["## 音声認識\n","\n","例えば、「電気をつけて」「電気を消して」などの音声による指示を受け付けるホームオートメーションシステムを構築したいとします。アプリケーションは、音声ベースの入力（音声による指示）を受け取り、それを解釈してテキストに変換し、それを解析して分析することができなければなりません。\n","\n","既に、音声を聞き取るための準備はできています。音声は、**マイク**または**オーディオファイル**から入力することができます。\n"]},{"cell_type":"markdown","metadata":{},"source":["### 音声ファイルによる音声認識\n","\n","以下のセルを実行すると、音声認識サービスが **音声ファイル** で動作する様子を見ることができます。\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from playsound import playsound\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n","\n","# Get spoken command from audio file\n","file_name = 'light-on.wav'\n","audio_file = os.path.join('data', 'speech', file_name)\n","\n","# Configure speech recognizer\n","speech_config = SpeechConfig(cog_key, cog_location, speech_recognition_language=language)\n","audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n","speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n","\n","# Use a one-time, synchronous call to transcribe the speech\n","speech = speech_recognizer.recognize_once()\n","\n","# Play the original audio file\n","playsound(audio_file)\n","\n","# Show transcribed text from audio file\n","print(speech.text)"]},{"cell_type":"markdown","metadata":{},"source":["## 音声合成\n","\n","さて、Speechサービスを使って音声をテキストに変換する方法をご紹介しましたが、その逆はどうでしょうか？テキストを音声に変換するにはどうすればよいのでしょうか。\n","\n","例えば、ホームオートメーションシステムが「ライトをつけて」というコマンドを解釈したとしましょう。適切な応答は、要求を口頭で確認することかもしれません（もちろん要求されたタスクを実際に実行することもね！）。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599695261170}},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n","%matplotlib inline\n","\n","# Get text to be spoken in English\n","response_text = 'Turning the light on.'\n","# Get text to be spoken in Japanese\n","# response_text = 'ライトを点けています.'\n","\n","# Configure speech synthesis\n","speech_config = SpeechConfig(cog_key, cog_location)\n","\n","# English Speech Synthesis\n","speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n","# Japanese Speech Synthesis \n","#speech_config.speech_synthesis_voice_name = 'ja-JP-NanamiNeural' # 'ja-JP-KeitaNeural'\n","\n","speech_synthesizer = SpeechSynthesizer(speech_config)\n","\n","# Transcribe text into speech\n","result = speech_synthesizer.speak_text(response_text)\n","\n","# Display an appropriate image \n","file_name = response_text.lower() + \"jpg\"\n","img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n","plt.axis('off')\n","plt. imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["変数 **response_text** を *Turning the light off.* (末尾のピリオドを含む)に変更し、セルを再度実行して結果を聞いてみてください。\n","\n","## さらに学ぶ\n","\n","このノートブックでは、Speech cognitive サービスを使用する非常に簡単な例をご覧いただきました。\n","Speechサービスについては、[音声テキスト変換のドキュメント](https://docs.microsoft.com/ja-jp/azure/cognitive-services/speech-service/index-speech-to-text)や[テキスト読み上げのドキュメント](https://docs.microsoft.com/ja-jp/azure/cognitive-services/speech-service/index-text-to-speech)で詳しく説明されています。"]}],"metadata":{"kernel_info":{"name":"python3-azureml"},"kernelspec":{"display_name":"Python 3.8.5 32-bit","metadata":{"interpreter":{"hash":"177429bd1865e7f7a0dbecbac90518c0d9641b1102b2e6c0df4b82dc948b5cb2"}},"name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}
