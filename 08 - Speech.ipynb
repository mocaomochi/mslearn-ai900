{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 音声の認識と合成\n","\n","私たちは、人工知能（AI）システムに話しかけることで、音声による応答を期待しながらコミュニケーションを取ることができるようになってきました。\n","\n","![A robot speaking](./images/speech.jpg)\n","\n","**音声認識**（音声言語を解釈するAIシステム）と**音声合成**（音声応答を生成するAIシステム）は、音声対応のAIソリューションの重要な構成要素です。\n","\n","## Cognitive Servicesリソースを作成する\n","\n","音声を解釈して応答するソフトウェアを構築するには、**Speech** Cognitive Serviceを使用することができます。このサービスは、音声言語をテキストに変換する簡単な方法を提供します。\n","\n","まだ作成していない方は、以下の手順でAzureサブスクリプションに**Cognitive Services**リソースを作成してください。\n","\n","> **補足** すでにCognitive Servicesリソースがある場合は、Azureポータルでその**クイックスタート**ページを開き、そのキーとエンドポイントを下のセルにコピーするだけです。そうでない場合は、以下の手順で作成してください。\n","\n","1. 新しいブラウザタブで、Azureポータル（https://portal.azure.com）を開き、Microsoftアカウントでサインインします。\n","2. **&#65291;リソースの作成** ボタンをクリックし、*Cognitive Services*を検索して以下の設定で **Cognitive Services** リソースを作成します。\n","    - **サブスクリプション**: *ご自身のサプスクリプション*\n","    - **リソースグループ**: *既存のリソースグループを選択するか、ユニークな名前で新しいリソースグループを作成します。*\n","    - **リージョン**: *利用可能なリージョンを選択（例:東日本）*\n","    - **名前**: *ユニークな名前を入力*\n","    - **価格レベル**: S0\n","    - **このボックスをオンにすることにより、以下のすべてのご契約条件を読み、同意したものとみなされます**: チェックを入れます。\n","3. デプロイが完了するまでしばらく待ちます。次に、Cognitive Servicesリソースにアクセスし、**Overview**ページで、サービスのキーを管理するリンクをクリックします。クライアントアプリケーションからCognitive Servicesリソースに接続するには、エンドポイントとキーが必要になります。\n","\n","### Cognitive Servicesリソースのキーとエンドポイントの取得\n","\n","Cognitive Servicesリソースを使用するためには、クライアントアプリケーションはそのエンドポイントと認証キーを必要とします。\n","\n","1. Azureポータルで、Cognitive Servicesリソースの**キーとエンドポイント**ページを開き、リソースの**キー1**をコピーして、**YOUR_COG_KEY**を置き換えて、以下のコードに貼り付けます。\n","\n","2. リソースの**エンドポイント**をコピーし、**YOUR_COG_ENDPOINT**を置き換えて、以下のコードに貼り付けます。\n","\n","3. セルの実行<span>&#9655;</span>ボタン（セルの左上）をクリックして、下のセルのコードを実行します。"]},{"cell_type":"code","execution_count":24,"metadata":{"gather":{"logged":1599695240794}},"outputs":[{"name":"stdout","output_type":"stream","text":["Ready to use cognitive services in https://mycognitiveservices-nw-2021.cognitiveservices.azure.com/ using key 2a87c574d93641378b396f8df1e66642. Speech language is set to ja-JP.\n"]}],"source":["#cog_key = 'YOUR_COG_KEY'\n","#cog_location = 'YOUR_COG_LOCATION'\n","cog_key = '2a87c574d93641378b396f8df1e66642'\n","cog_location = 'https://mycognitiveservices-nw-2021.cognitiveservices.azure.com/'\n","language = 'ja-JP'\n","region ='westus'\n","print('Ready to use cognitive services in {} using key {}. Speech language is set to {}.'.format(cog_location, cog_key, language))"]},{"cell_type":"markdown","metadata":{},"source":["## 音声認識\n","\n","例えば、「電気をつけて」「電気を消して」などの音声による指示を受け付けるホームオートメーションシステムを構築したいとします。アプリケーションは、音声ベースの入力（音声による指示）を受け取り、それを解釈してテキストに変換し、それを解析して分析することができなければなりません。\n","\n","既に、音声を聞き取るための準備はできています。音声は、**マイク**または**オーディオファイル**から入力することができます。\n"]},{"cell_type":"markdown","metadata":{},"source":["### 音声ファイルによる音声認識\n","\n","以下のセルを実行すると、音声認識サービスが **音声ファイル** で動作する様子を見ることができます。\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ライトをつけて。\n"]}],"source":["import os\n","from playsound import playsound\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n","\n","# Get spoken command from audio file\n","file_name = 'light-on-jp.wav'\n","audio_file = os.path.join('data', 'speech', file_name)\n","\n","# Configure speech recognizer\n","speech_config = SpeechConfig(cog_key, region, speech_recognition_language=language)\n","# speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n","audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n","speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n","\n","# Use a one-time, synchronous call to transcribe the speech\n","speech = speech_recognizer.recognize_once()\n","\n","# Play the original audio file\n","playsound(audio_file)\n","\n","# Show transcribed text from audio file\n","print(speech.text)"]},{"cell_type":"markdown","metadata":{},"source":["## 音声合成\n","\n","さて、Speechサービスを使って音声をテキストに変換する方法をご紹介しましたが、その逆はどうでしょうか？テキストを音声に変換するにはどうすればよいのでしょうか。\n","\n","例えば、ホームオートメーションシステムが「ライトをつけて」というコマンドを解釈したとしましょう。適切な応答は、要求を口頭で確認することかもしれません（もちろん要求されたタスクを実際に実行することもね！）。"]},{"cell_type":"code","execution_count":29,"metadata":{"gather":{"logged":1599695261170}},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/speech/ライトを点けています。jpg'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/yt/vc6b82qj5nl8jq612h2rxpm80000gn/T/ipykernel_20006/10966455.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Display an appropriate image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"speech\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.12/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/speech/ライトを点けています。jpg'"]}],"source":["import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n","%matplotlib inline\n","\n","# Get text to be spoken in English\n","#response_text = 'Turning the light on.'\n","# Get text to be spoken in Japanese\n","response_text = 'ライトを点けています。'\n","\n","# Configure speech synthesis\n","speech_config = SpeechConfig(cog_key, region)\n","\n","# English Speech Synthesis\n","#speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n","\n","# Japanese Speech Synthesis \n","speech_config.speech_synthesis_voice_name = 'ja-JP-NanamiNeural' # 'ja-JP-KeitaNeural'\n","\n","speech_synthesizer = SpeechSynthesizer(speech_config)\n","\n","# Transcribe text into speech\n","result = speech_synthesizer.speak_text(response_text)\n","\n","# Display an appropriate image \n","file_name = response_text.lower() + \"jpg\"\n","img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n","plt.axis('off')\n","plt. imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["Try changing the **response_text** variable to *Turning the light off.* (including the period at the end) and run the cell again to hear the result.\n","\n","## Learn more\n","\n","You've seen a very simple example of using the Speech cognitive service in this notebook. You can learn more about [speech-to-text](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text) and [text-to-speech](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech) in the Speech service documentation."]}],"metadata":{"kernel_info":{"name":"python3-azureml"},"kernelspec":{"display_name":"Python 3.8.5 32-bit","metadata":{"interpreter":{"hash":"177429bd1865e7f7a0dbecbac90518c0d9641b1102b2e6c0df4b82dc948b5cb2"}},"name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}
