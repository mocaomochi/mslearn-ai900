{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 光学式文字認識\n","\n","![A robot reading a newspaper](./images/ocr.jpg)\n","\n","コンピュータビジョンでは、画像中の文字を検出して解釈することがよく行われます。このような処理は、**光学的文字認識**（OCR）と呼ばれています。\n","\n","## Computer Visionサービスを利用した画像中の文字の読み取り\n","\n","Computer Vision Cognitive Service は、以下のようなOCRタスクをサポートします。\n","\n","- 複数の言語のテキストを読み取るために使用できる **OCR** API。このAPIは、同期して動作し、画像内の少量のテキストを検出して読み取る必要がある場合に有効です。\n","\n","- 大きなドキュメントに最適化された **Read** API。このAPIは非同期で動作し、印刷されたテキストと手書きのテキストの両方に使用することができます。\n","\n","これらのサービスを利用するには、**Computer Vision**リソースまたは**Cognitive Services**リソースを作成する必要があります。\n","\n","まだCognitive Servicesリソースの作成を行っていない場合は、Azureサブスクリプションに**Cognitive Services**リソースを作成します。\n","\n","> **補足**　すでにCognitive Servicesリソースがある場合は、Azureポータルでその**クイックスタート**ページを開き、そのキーとエンドポイントを下のセルにコピーするだけです。そうでない場合は、以下の手順で作成してください。\n","\n","1. 新しいブラウザタブで、Azureポータル（https://portal.azure.com）を開き、Microsoftアカウントでサインインします。\n","\n","2. **&#65291;リソースの作成** ボタンをクリックし、*Cognitive Services*を検索して以下の設定で **Cognitive Services** リソースを作成します。\n","    - **サブスクリプション**: *ご自身のサプスクリプション*\n","    - **リソースグループ**: *既存のリソースグループを選択するか、ユニークな名前で新しいリソースグループを作成します。*\n","    - **リージョン**: *利用可能なリージョンを選択（例:東日本）*\n","    - **名前**: *ユニークな名前を入力*\n","    - **価格レベル**: S0\n","    - **このボックスをオンにすることにより、以下のすべてのご契約条件を読み、同意したものとみなされます**: チェックを入れます。\n","3. デプロイが完了するまでしばらく待ちます。次に、Cognitive Servicesリソースにアクセスし、**Overview**ページで、サービスのキーを管理するリンクをクリックします。クライアントアプリケーションからCognitive Servicesリソースに接続するには、エンドポイントとキーが必要になります。\n","\n","### Cognitive Servicesリソースのキーとエンドポイントの取得\n","\n","Cognitive Servicesリソースを使用するためには、クライアントアプリケーションはそのエンドポイントと認証キーを必要とします。\n","\n","1. Azureポータルで、Cognitive Servicesリソースの**キーとエンドポイント**ページを開き、リソースの**キー1**をコピーして、**YOUR_COG_KEY**を置き換えて、以下のコードに貼り付けます。\n","\n","2. リソースの**エンドポイント**をコピーし、**YOUR_COG_ENDPOINT**を置き換えて、以下のコードに貼り付けます。\n","\n","3. セルの実行<span>&#9655;</span>ボタン（セルの左上）をクリックして、下のセルのコードを実行します。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599694246277}},"outputs":[],"source":["cog_key = 'YOUR_COG_KEY'\n","cog_endpoint = 'YOUR_COG_ENDPOINT'\n","\n","print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["キーとエンドポイントを設定したので、Computer Visionサービスのリソースを使って、画像からテキストを抽出することができます。\n","\n","まず、**OCR** APIを使って、画像を同期的に分析し、含まれるテキストを読み取ることができます。このケースでは、架空の小売会社「Northwind Traders」の広告画像に、いくつかのテキストが含まれています。以下のセルを実行して、テキストを読んでみましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599694257280}},"outputs":[],"source":["from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n","from msrest.authentication import CognitiveServicesCredentials\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageDraw\n","import os\n","%matplotlib inline\n","\n","# Get a client for the computer vision service\n","computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n","\n","# Read the image file\n","image_path = os.path.join('data', 'ocr', 'advert.jpg')\n","image_stream = open(image_path, \"rb\")\n","\n","# Use the Computer Vision service to find text in the image\n","read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n","\n","# Process the text line by line\n","for region in read_results.regions:\n","    for line in region.lines:\n","\n","        # Read the words in the line of text\n","        line_text = ''\n","        for word in line.words:\n","            line_text += word.text + ' '\n","        print(line_text.rstrip())\n","\n","# Open image to display it.\n","fig = plt.figure(figsize=(7, 7))\n","img = Image.open(image_path)\n","draw = ImageDraw.Draw(img)\n","plt.axis('off')\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["画像の中のテキストは、領域、行、単語の階層構造になっており、コードはこれらを読み取って結果を取得します。\n","\n","結果では、画像の上に読み込まれたテキストを表示します。\n","\n","## 境界ボックスの表示\n","\n","結果には、画像の中にあるテキストの行や個々の単語の**境界ボックス**座標も含まれています。以下のセルを実行して、上記で取得した広告画像のテキスト行の境界ボックスを確認してください。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599694266106}},"outputs":[],"source":["# Open image to display it.\n","fig = plt.figure(figsize=(7, 7))\n","img = Image.open(image_path)\n","draw = ImageDraw.Draw(img)\n","\n","# Process the text line by line\n","for region in read_results.regions:\n","    for line in region.lines:\n","\n","        # Show the position of the line of text\n","        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n","        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n","\n","        # Read the words in the line of text\n","        line_text = ''\n","        for word in line.words:\n","            line_text += word.text + ' '\n","        print(line_text.rstrip())\n","\n","# Show the image with the text locations highlighted\n","plt.axis('off')\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["出力結果には、テキストの各行の境界ボックスが画像上に長方形として表示されます。\n","\n","## Read APIの使用\n","\n","以前に使用したOCR APIは、少量のテキストを含む画像に対してはうまく機能します。スキャンされた文書など、より大きなテキストを読み取る必要がある場合は、 **Read** APIを使用することができます。これには、いくつかのステップが必要です。\n","\n","1. Computer Visionサービスに画像を送信し、非同期的に読み取りと分析を行う。\n","   \n","2. 解析処理が完了するのを待つ。\n","   \n","3. 解析結果を取得します。\n","\n","次のセルを実行して、このプロセスを使用して、Northwind Traders ストアのマネージャーに宛てたスキャンされた手紙のテキストを読み取ります。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599694312346}},"outputs":[],"source":["from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n","from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n","from msrest.authentication import CognitiveServicesCredentials\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import time\n","import os\n","%matplotlib inline\n","\n","# Read the image file\n","image_path = os.path.join('data', 'ocr', 'letter.jpg')\n","image_stream = open(image_path, \"rb\")\n","\n","# Get a client for the computer vision service\n","computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n","\n","# Submit a request to read printed text in the image and get the operation ID\n","read_operation = computervision_client.read_in_stream(image_stream,\n","                                                      raw=True)\n","operation_location = read_operation.headers[\"Operation-Location\"]\n","operation_id = operation_location.split(\"/\")[-1]\n","\n","# Wait for the asynchronous operation to complete\n","while True:\n","    read_results = computervision_client.get_read_result(operation_id)\n","    if read_results.status not in [OperationStatusCodes.running]:\n","        break\n","    time.sleep(1)\n","\n","# If the operation was successfuly, process the text line by line\n","if read_results.status == OperationStatusCodes.succeeded:\n","    for result in read_results.analyze_result.read_results:\n","        for line in result.lines:\n","            print(line.text)\n","\n","# Open image and display it.\n","print('\\n')\n","fig = plt.figure(figsize=(12,12))\n","img = Image.open(image_path)\n","plt.axis('off')\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["結果を確認する。ほとんどが印刷されたテキストと手書きの署名で構成された手紙の完全なトランスクリプションがあります。OCR結果の下には、手紙のオリジナル画像が表示されています（スクロールしないと表示されない場合があります）。\n","\n","## 手書きのテキストを読む\n","\n","前述の例では、画像の解析要求で、**印刷された**テキストに最適化されたテキスト認識モードが指定されていました。それにもかかわらず、手書きの署名が読み取れたことに注目してください。\n","\n","このように、手書きのテキストを読み取る機能は非常に便利です。例えば、買い物リストが書かれたメモを書き、スマホのアプリを使ってそのメモを読み、含まれているテキストを書き起こしたいとしましょう。\n","\n","以下のセルを実行して、手書きの買い物リストの読み取り操作の例を見てみましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1599694340593}},"outputs":[],"source":["from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n","from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n","from msrest.authentication import CognitiveServicesCredentials\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import time\n","import os\n","%matplotlib inline\n","\n","# Read the image file\n","image_path = os.path.join('data', 'ocr', 'note.jpg')\n","image_stream = open(image_path, \"rb\")\n","\n","# Get a client for the computer vision service\n","computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n","\n","# Submit a request to read printed text in the image and get the operation ID\n","read_operation = computervision_client.read_in_stream(image_stream,\n","                                                      raw=True)\n","operation_location = read_operation.headers[\"Operation-Location\"]\n","operation_id = operation_location.split(\"/\")[-1]\n","\n","# Wait for the asynchronous operation to complete\n","while True:\n","    read_results = computervision_client.get_read_result(operation_id)\n","    if read_results.status not in [OperationStatusCodes.running]:\n","        break\n","    time.sleep(1)\n","\n","# If the operation was successfuly, process the text line by line\n","if read_results.status == OperationStatusCodes.succeeded:\n","    for result in read_results.analyze_result.read_results:\n","        for line in result.lines:\n","            print(line.text)\n","\n","# Open image and display it.\n","print('\\n')\n","fig = plt.figure(figsize=(12,12))\n","img = Image.open(image_path)\n","plt.axis('off')\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["## さらに学ぶ\n","\n","OCRのComputer Visionサービスの使用についてより詳しい情報は、[光学式文字認識とは](https://docs.microsoft.com/ja-jp/azure/cognitive-services/computer-vision/concept-recognizing-text)を参照してください。"]}],"metadata":{"interpreter":{"hash":"887aba159db394e56c0af5a7ac88b171e7a8c319799b3662e2d20ed0d5bb4a15"},"kernel_info":{"name":"python3-azureml"},"kernelspec":{"display_name":"Python 3.6 - AzureML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}
